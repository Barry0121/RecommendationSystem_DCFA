{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "import json\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# put the \"dataset\" folder in the root directory\n",
    "base = 'dataset'\n",
    "if base not in os.listdir('.'):\n",
    "    print(\"dataset folder not found.\")\n",
    "    # break # made available in the final py file\n",
    "else:\n",
    "    interactions_Jewelry_train = os.path.join(base, 'interactions_Jewelry_train.json')\n",
    "    interactions_Jewelry_train_aux = os.path.join(base, 'interactions_Jewelry_train_aux.json')\n",
    "    interactions_Jewelry_train_record = os.path.join(base, 'interactions_Jewelry_train_record_aux.json')\n",
    "    interactions_Jewelry_train_time = os.path.join(base, 'interactions_Jewelry_train_time_aux.json')\n",
    "    interactions_Jewelry_validate = os.path.join(base, 'interactions_Jewelry_validate.json')\n",
    "    interactions_Jewelry_test = os.path.join(base, 'interactions_Jewelry_test.json')\n",
    "CNN_AES = \"CNN_AES_feature.txt\"\n",
    "id2num_dict = \"id2num_dict_Jewelry.json\"\n",
    "if CNN_AES not in os.listdir(os.path.join('.', base, \"features\")) or id2num_dict not in os.listdir(os.path.join('.', base, \"id2num_dict\")):\n",
    "    print(\"CNN_AES and id2num_dict folders not found.\")\n",
    "    # break\n",
    "else:\n",
    "    CNN_AES = os.path.join(base, \"features\", CNN_AES)\n",
    "    id2num_dict = os.path.join(base, \"id2num_dict\", id2num_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Storing User, Item, Time tuples in `interactionTrain`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "userIDs = set()\n",
    "itemIDs = set()\n",
    "#interactionsTrain = []\n",
    "user_to_item = {}\n",
    "user_time_to_item = {}\n",
    "time_to_item = {}\n",
    "\n",
    "with open(interactions_Jewelry_train) as json_file:\n",
    "    data = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for d in data[:50]:\n",
    "    u = d[0]\n",
    "    i = d[1]\n",
    "    r = d[2]\n",
    "    #interactionsTrain.append((u,i,r))\n",
    "    userIDs.add(u)\n",
    "    itemIDs.add(i)\n",
    "    if u in user_to_item:\n",
    "        user_to_item[u].add(i)\n",
    "    else:\n",
    "        user_to_item[u] = {i}\n",
    "    if (u,r) in user_time_to_item:\n",
    "        user_time_to_item[(u,r)].add(i)\n",
    "    else:\n",
    "        user_time_to_item[(u,r)] = {i}\n",
    "    if r in time_to_item:\n",
    "        time_to_item[r].add(i)\n",
    "    else:\n",
    "        time_to_item[r] = {i}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Storing CNN-AES Feature in `cnn`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open(CNN_AES) as cnn_txt:\n",
    "    cnn = cnn_txt.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read id2num_dict in which contains item id and the dictionary index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with open(id2num_dict) as id2num_dict_json:\n",
    "    id2num = id2num_dict_json.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*From Library.py*\n",
    "Functions for:\n",
    "- Model Evaluation\n",
    "- Data Reading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "from xlrd import open_workbook\n",
    "from xlutils.copy import copy\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "Evaluation Metrics\n",
    "\"\"\"\n",
    "\n",
    "def evaluation_F1(order, top_k, positive_item):\n",
    "    e = 0.00000000000001\n",
    "    top_k_items = set(order[0: top_k])\n",
    "    positive_item = set(positive_item)\n",
    "    precision = len(top_k_items & positive_item) / (len(top_k_items) + e)\n",
    "    recall = len(top_k_items & positive_item) / (len(positive_item) + e)\n",
    "    F1 = 2 * precision * recall / (precision + recall + e)\n",
    "    return F1\n",
    "\n",
    "def evaluation_NDCG(order, top_k, positive_item):\n",
    "    top_k_item = order[0: top_k]\n",
    "    e = 0.0000000001\n",
    "    Z_u = 0\n",
    "    temp = 0\n",
    "    for i in range(0, top_k):\n",
    "        Z_u += 1 / log2(i + 2)\n",
    "        if top_k_item[i] in positive_item:\n",
    "            temp += 1 / log2(i + 2)\n",
    "    NDCG = temp / (Z_u + e)\n",
    "    return NDCG\n",
    "\n",
    "def save_result(intro, F1, NDCG, path):\n",
    "    rexcel = open_workbook(path)\n",
    "    rows = rexcel.sheets()[0].nrows\n",
    "    excel = copy(rexcel)\n",
    "    table = excel.get_sheet(0)\n",
    "    row = rows\n",
    "    table.write(row, 0, intro)\n",
    "    #table.write(row, 2, 'F1')\n",
    "    for i in range(len(F1)):\n",
    "        table.write(row, i + 3, F1[i])\n",
    "    #table.write(row, len(F1) + 4, 'NDCG')\n",
    "    for i in range(len(NDCG)):\n",
    "        table.write(row, i + len(F1) + 5, NDCG[i])\n",
    "    excel.save(path)\n",
    "\n",
    "\"\"\"\n",
    "Read Data\n",
    "\"\"\"\n",
    "def readdata(dataset):\n",
    "    #file paths\n",
    "    path_train = interactions_Jewelry_train\n",
    "    path_train_aux = interactions_Jewelry_train_aux\n",
    "    path_validate = interactions_Jewelry_validate\n",
    "    path_test = interactions_Jewelry_test\n",
    "    # read files\n",
    "    with open(path_train) as f:\n",
    "        line = f.readline()\n",
    "        train_data = json.loads(line)\n",
    "    f.close()\n",
    "    P = 0\n",
    "    Q = 0\n",
    "    for [u, i, r] in train_data:\n",
    "        if u > P:\n",
    "            P = u\n",
    "        if i > Q:\n",
    "            Q = i\n",
    "    with open(path_train_aux) as f:\n",
    "        line = f.readline()\n",
    "        train_data_aux = json.loads(line)\n",
    "    f.close()\n",
    "    with open(path_validate) as f:\n",
    "        line = f.readline()\n",
    "        validate_data = json.loads(line)\n",
    "    f.close()\n",
    "    with open(path_test) as f:\n",
    "        line = f.readline()\n",
    "        test_data = json.loads(line)\n",
    "    f.close()\n",
    "    return train_data, train_data_aux, validate_data, test_data, P + 1, Q + 1 # P: last user_id, last item_id\n",
    "\n",
    "def readdata_time(dataset):\n",
    "    #file paths\n",
    "    path_train_record_aux = interactions_Jewelry_train_record\n",
    "    path_train_time_aux = interactions_Jewelry_train_time\n",
    "    # read files\n",
    "    with open(path_train_record_aux) as f:\n",
    "        line = f.readline()\n",
    "        train_record_aux = json.loads(line)\n",
    "    f.close()\n",
    "    with open(path_train_time_aux) as f:\n",
    "        line = f.readline()\n",
    "        train_time_aux = json.loads(line)\n",
    "    f.close()\n",
    "    return train_record_aux, train_time_aux, len(train_time_aux)\n",
    "\n",
    "def read_feature(feature, dataset, Q):\n",
    "    path_feature = CNN_AES\n",
    "    path_dict = id2num_dict\n",
    "    with open(path_dict) as f:\n",
    "        line = f.readline()\n",
    "        item_i2num_dict = json.loads(line)\n",
    "    f.close()\n",
    "    f = open(path_feature, 'r')\n",
    "    line = eval(f.readline())\n",
    "    feature = line[1]\n",
    "    K = len(feature)\n",
    "    F = np.zeros((Q, K))\n",
    "    for i in range(0, Q):\n",
    "        F[i] = feature\n",
    "    for line in f:\n",
    "        line = eval(line)\n",
    "        item_id = line[0]\n",
    "        feature = line[1]\n",
    "        try:\n",
    "            item_num = item_i2num_dict[item_id]\n",
    "            F[item_num] = feature\n",
    "        except:\n",
    "            continue\n",
    "    return F\n",
    "\n",
    "def get_feature(dataset):\n",
    "    # to load features\n",
    "    feat_list = ['CNN', 'AES', 'CH', 'CNN_AES']             # feature list\n",
    "    F = read_feature(feat_list[feat[0]], dataset, Q)\n",
    "    for i in range(1, len(feat)):\n",
    "        F = np.hstack((F, read_feature(feat_list[feat[i]], dataset, Q)))\n",
    "    return F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# setup\n",
    "feat = [3]                          # feature selecting, 0 for CNN, 1 for AES, 2 for CH, 3 for CNN+AES\n",
    "dataset = 5                         # Datasets selecting 0 to 5 for 'All', '_Women', '_Men', '_CLothes', '_Shoes', '_Jewelry' respectively\n",
    "dataset_list = ['', '_Women', '_Men', '_CLothes', '_Shoes', '_Jewelry']\n",
    "# load data\n",
    "train_data, train_data_aux, validate_data, test_data, P, Q = readdata(dataset_list[dataset])\n",
    "# load data for tensor factorization\n",
    "train_record_aux, train_time_aux, R = readdata_time(dataset_list[dataset])\n",
    "# load features\n",
    "F = get_feature(dataset_list[dataset]) # CNN_AES Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DCFA(tf.keras.Model):\n",
    "    def __init__(self, P, Q, R, I, J, F, reg=1.5, mom=0.1):\n",
    "        \"\"\"\n",
    "        Initialization of the Visually_based Bayesian Personalized Ranking Model with Aesthetic Features\n",
    "\n",
    "        :param P: Number of Users\n",
    "        :param Q: Number of Items\n",
    "        :param R: Number of Time Intervals\n",
    "        :param I: Dimension of each Latent Items\n",
    "        :param J: Dimension of each Latent Items\n",
    "        :param F: The CNN-AES Feature Matrix\n",
    "        :param reg: regularize lambda\n",
    "        :param mom: momentum gamma\n",
    "        \"\"\"\n",
    "        super(DCFA, self).__init__()\n",
    "        # Latent Items\n",
    "        self.U = tf.Variable(np.array([np.array([(random.random() / math.sqrt(I)) for j in range(I)]) for i in range(P)]))\n",
    "        print(\"Dimension of U: \", self.U.shape)\n",
    "        self.V = tf.Variable(np.array([np.array([(random.random() / math.sqrt(I)) for j in range(I)]) for i in range(Q)]))\n",
    "        print(\"Dimension of V: \", self.V.shape)\n",
    "        self.W = tf.Variable(np.array([np.array([(random.random() / math.sqrt(J)) for j in range(J)]) for i in range(Q)]))\n",
    "        print(\"Dimension of W: \", self.W.shape)\n",
    "        self.T = tf.Variable(np.array([np.array([(random.random() / math.sqrt(J)) for j in range(J)]) for i in range(R)]))\n",
    "        print(\"Dimension of T: \", self.T.shape)\n",
    "        # Extract CNN-AES Features\n",
    "        self.F, self.K = F, len(F[0])\n",
    "        print(\"Dimension of F: \", self.F.shape)\n",
    "        print(\"Size of F: \", self.K)\n",
    "        self.M = tf.Variable(np.array([np.array([(random.random() / math.sqrt(self.K)) for j in range(self.K)]) for i in range(P)]))\n",
    "        print(\"Dimension of M: \", self.M.shape)\n",
    "        self.N = tf.Variable(np.array([np.array([(random.random() / math.sqrt(self.K)) for j in range(self.K)]) for i in range(R)]))\n",
    "        print(\"Dimension of N: \", self.N.shape)\n",
    "        # regularize coefficient and momentum coefficient\n",
    "        self.reg = reg\n",
    "        self.mom = mom\n",
    "        # self.top_k = [5, 10, 20, 50, 100]\n",
    "\n",
    "    def score(self, u, v, r):\n",
    "        \"\"\"\n",
    "        Given a (user, item, time) tuple, return the score associated with the relevancy of the given item to given user\n",
    "        at given timestamp\n",
    "\n",
    "        :param u: user_id\n",
    "        :param v: item_id\n",
    "        :param r: timestamp\n",
    "        :return: BPR score for the (user, item, time) tuple\n",
    "        \"\"\"\n",
    "        # B = (tf.expand_dims(self.U[u], axis=0) @ tf.transpose(self.V) + tf.expand_dims(self.M[u], axis=0) @ tf.transpose(self.F))[0][v]\n",
    "        # C = (tf.expand_dims(self.T[r], axis=0) @ tf.transpose(self.W) + tf.expand_dims(self.N[r], axis=0) @ tf.transpose(self.F))[0][v]\n",
    "        #print(B.shape, C.shape)\n",
    "        #return tf.tensordot(B, C, axes=0), B, C\n",
    "        # print(\"score\" + str(B*C))\n",
    "        # return B*C\n",
    "\n",
    "        B = tf.expand_dims(self.U[u], axis=0) @ tf.transpose(self.V) + tf.expand_dims(self.M[u], axis=0) @ tf.transpose(self.F)\n",
    "        C = tf.expand_dims(self.T[r], axis=0) @ tf.transpose(self.W) + tf.expand_dims(self.N[r], axis=0) @ tf.transpose(self.F)\n",
    "        print(\"B Shape: \", B.shape)\n",
    "        print(\"C Shape: \", C.shape)\n",
    "        print(\"A Shape: \", (tf.squeeze(B)*tf.squeeze(C)).shape)\n",
    "        return (tf.squeeze(B)*tf.squeeze(C))[v], tf.squeeze(B)[v], tf.squeeze(C)[v]\n",
    "\n",
    "    def score_batch(self, u, r):\n",
    "        \"\"\"\n",
    "        Given a (user, time) pair, return an array of scores associated with the given user and given time\n",
    "        :param u:\n",
    "        :param r:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        B = tf.expand_dims(self.U[u], axis=0) @ tf.transpose(self.V) + tf.expand_dims(self.M[u], axis=0) @ tf.transpose(self.F)\n",
    "        C = tf.expand_dims(self.T[r], axis=0) @ tf.transpose(self.W) + tf.expand_dims(self.N[r], axis=0) @ tf.transpose(self.F)\n",
    "        return tf.squeeze(B)*tf.squeeze(C), tf.squeeze(B), tf.squeeze(C)\n",
    "\n",
    "    def call(self, u, v, v_prime, r):\n",
    "        \"\"\"\n",
    "        Calculate the BPR_OPT distance, which is a metric that measures the scoring distance between positive sample\n",
    "        and the negative sample of the given (user, item, time) tuple\n",
    "\n",
    "        :param u: user_id\n",
    "        :param v: item_id\n",
    "        :param v_prime: negative sample of item_id\n",
    "        :param r: timestamp\n",
    "        :return: loss value for the given tuple\n",
    "        \"\"\"\n",
    "        A_i, B_i, C_i = self.score(u, v, r)\n",
    "        A_j, B_j, C_j = self.score(u, v_prime, r)\n",
    "\n",
    "        print(\"B_ij: \", B_i-B_j)\n",
    "        print(\"C_ij: \", C_i-C_j)\n",
    "\n",
    "        A_loss = np.log(tf.keras.activations.sigmoid(A_i-A_j))\n",
    "        B_loss = np.log(tf.keras.activations.sigmoid(B_i-B_j))\n",
    "        C_loss = np.log(tf.keras.activations.sigmoid(C_i-C_j))\n",
    "        # B_loss = tf.keras.activations.sigmoid(B_i-B_j)\n",
    "        # C_loss = tf.keras.activations.sigmoid(C_i-C_j)\n",
    "\n",
    "        print(\"A loss: \", A_loss)\n",
    "        print(\"B loss: \", B_loss)\n",
    "        print(\"C loss: \", C_loss)\n",
    "\n",
    "        return A_loss + self.mom * B_loss + self.mom * C_loss\n",
    "        # TODO: Fix Lambda regularization term (i.e. self.reg and self.mom)\n",
    "\n",
    "    def reg(self):\n",
    "        \"\"\"\n",
    "        Return the regularization value for the current latent terms\n",
    "\n",
    "        :return: regularization term\n",
    "        \"\"\"\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) + tf.reduce_sum(self.betaI**2) +\n",
    "                            tf.reduce_sum(self.gammaU**2) + tf.reduce_sum(self.gammaI**2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "# Negative sample\n",
    "item_size = len(itemIDs)\n",
    "index = randrange(item_size)\n",
    "itemIDs_list = list(itemIDs)\n",
    "negative_item = itemIDs_list[index]\n",
    "\n",
    "while negative_item in user_to_item[12308]:\n",
    "    item_size = len(itemIDs)\n",
    "    index = randrange(item_size)\n",
    "    negative_item = itemIDs_list[index]\n",
    "    del itemIDs_list[index]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Engine.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class engine():\n",
    "    def __init__(self, learning_rate=1e-3, batch_size=64, k=5, feature=('CNN')):\n",
    "        \"\"\"\n",
    "\n",
    "        :param learning_rate:\n",
    "        :param batch_size:\n",
    "        :param k:\n",
    "        :param feature: tuple of strings, default: ['CNN'], alternatives: ['CNN', 'AES', 'CH', 'CNN_AES']\n",
    "        \"\"\"\n",
    "        self.optimizer = None\n",
    "        self.model = None\n",
    "\n",
    "        # define hyperparameter\n",
    "        self.k = k\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.feat = feature\n",
    "\n",
    "        # save data\n",
    "        self.userIDs = set()\n",
    "        self.itemIDs = set()\n",
    "        self.user_to_item = {}\n",
    "        self.user_time_to_item = {}\n",
    "        self.time_to_item = {}\n",
    "        self.cnn = None\n",
    "        self.id2num = None\n",
    "        self.train_data = None\n",
    "        self.train_data_aux = None\n",
    "        self.validate_data = None\n",
    "        self.test_data = None\n",
    "        self.train_record_aux = None\n",
    "        self.train_time_aux = None\n",
    "        self.R = 0\n",
    "        self.P = 0\n",
    "        self.Q = 0\n",
    "        self.F = None\n",
    "        self.K = None\n",
    "\n",
    "        # Fill data structures\n",
    "        self.interactions_Jewelry_train, self.interactions_Jewelry_train_aux, self.interactions_Jewelry_train_record, self.interactions_Jewelry_train_time, self.interactions_Jewelry_validate, self.interactions_Jewelry_test, self.CNN_AES, self.id2num_dict = self.read_data()\n",
    "\n",
    "        self.read_feature_data(self.Q)\n",
    "\n",
    "        # Initialize Dataset\n",
    "        self.train_data, self.test_data = self.read_interaction_data(self.interactions_Jewelry_train), self.read_interaction_data(self.interactions_Jewelry_test)\n",
    "\n",
    "        self.create_model()\n",
    "\n",
    "    def train_batch(self):\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "    def test_batch(self):\n",
    "        # TODO:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def read_data(self, base='dataset'):\n",
    "        # put the \"dataset\" folder in the root directory\n",
    "        base = base\n",
    "        if base not in os.listdir('.'):\n",
    "            print(\"dataset folder not found.\")\n",
    "            # break # made available in the final py file\n",
    "        else:\n",
    "            interactions_Jewelry_train = os.path.join(base, 'interactions_Jewelry_train.json')\n",
    "            interactions_Jewelry_train_aux = os.path.join(base, 'interactions_Jewelry_train_aux.json')\n",
    "            interactions_Jewelry_train_record = os.path.join(base, 'interactions_Jewelry_train_record_aux.json')\n",
    "            interactions_Jewelry_train_time = os.path.join(base, 'interactions_Jewelry_train_time_aux.json')\n",
    "            interactions_Jewelry_validate = os.path.join(base, 'interactions_Jewelry_validate.json')\n",
    "            interactions_Jewelry_test = os.path.join(base, 'interactions_Jewelry_test.json')\n",
    "        CNN_AES = \"CNN_AES_feature.txt\"\n",
    "        id2num_dict = \"id2num_dict_Jewelry.json\"\n",
    "\n",
    "        if CNN_AES not in os.listdir(os.path.join('.', base, \"features\")) or id2num_dict not in os.listdir(os.path.join('.', base, \"id2num_dict\")):\n",
    "            print(\"CNN_AES and id2num_dict folders not found.\")\n",
    "            # break\n",
    "        else:\n",
    "            CNN_AES = os.path.join(base, \"features\", CNN_AES)\n",
    "            id2num_dict = os.path.join(base, \"id2num_dict\", id2num_dict)\n",
    "        return interactions_Jewelry_train, interactions_Jewelry_train_aux, interactions_Jewelry_train_record, interactions_Jewelry_train_time, interactions_Jewelry_validate, interactions_Jewelry_test, CNN_AES, id2num_dict\n",
    "\n",
    "\n",
    "    def read_interaction_data(self, filePath, data_size=50):\n",
    "        print(f\"Start reading the interaction data {filePath}...\")\n",
    "        with open(filePath) as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        for d in data[:data_size]:\n",
    "            u = d[0]\n",
    "            i = d[1]\n",
    "            r = d[2]\n",
    "            #interactionsTrain.append((u,i,r))\n",
    "\n",
    "            self.userIDs.add(u)\n",
    "            if type(i) == list:\n",
    "                i = i[0]\n",
    "            self.itemIDs.add(i)\n",
    "            if type(r) == list:\n",
    "                r = r[0]\n",
    "            if u in self.user_to_item:\n",
    "                self.user_to_item[u].add(i)\n",
    "            else:\n",
    "                self.user_to_item[u] = {i}\n",
    "            if (u,r) in self.user_time_to_item:\n",
    "                self.user_time_to_item[(u,r)].add(i)\n",
    "            else:\n",
    "                self.user_time_to_item[(u,r)] = {i}\n",
    "            if r in self.time_to_item:\n",
    "                self.time_to_item[r].add(i)\n",
    "            else:\n",
    "                self.time_to_item[r] = {i}\n",
    "\n",
    "    def read_feature(self, feature, dataset, Q):\n",
    "        path_feature = self.CNN_AES\n",
    "        path_dict = self.id2num_dict\n",
    "        with open(path_dict) as f:\n",
    "            line = f.readline()\n",
    "            item_i2num_dict = json.loads(line)\n",
    "        f.close()\n",
    "        f = open(path_feature, 'r')\n",
    "        line = eval(f.readline())\n",
    "        feature = line[1]\n",
    "        self.K = len(feature)\n",
    "        F = np.zeros((Q, self.K))\n",
    "        for i in range(0, Q):\n",
    "            F[i] = feature\n",
    "        for line in f:\n",
    "            line = eval(line)\n",
    "            item_id = line[0]\n",
    "            feature = line[1]\n",
    "            try:\n",
    "                item_num = item_i2num_dict[item_id]\n",
    "                F[item_num] = feature\n",
    "            except:\n",
    "                continue\n",
    "        return F\n",
    "\n",
    "    def get_feature(self, dataset, Q):\n",
    "\n",
    "        # feat_list = ['CNN', 'AES', 'CH', 'CNN_AES']             # feature list\n",
    "        F = self.read_feature(self.feat[0], dataset, Q)\n",
    "        for i in range(1, len(self.feat)):\n",
    "            F = np.hstack((F, self.read_feature(self.feat[i], dataset, Q)))\n",
    "        return F\n",
    "\n",
    "    def read_feature_data(self, Q):\n",
    "        \"\"\"\n",
    "        Main Feature Loading Function\n",
    "\n",
    "        :param Q: number of users\n",
    "        \"\"\"\n",
    "        print(\"Start getting feature data...\")\n",
    "        with open(self.CNN_AES) as cnn_txt:\n",
    "            self.cnn = cnn_txt.readlines()\n",
    "        with open(self.id2num_dict) as id2num_dict_json:\n",
    "            self.id2num = id2num_dict_json.readlines()\n",
    "\n",
    "        # setup\n",
    "        feat = [3]                          # feature selecting, 0 for CNN, 1 for AES, 2 for CH, 3 for CNN+AES\n",
    "        dataset = 5                         # Datasets selecting 0 to 5 for 'All', '_Women', '_Men', '_CLothes', '_Shoes', '_Jewelry' respectively\n",
    "        dataset_list = ['', '_Women', '_Men', '_Clothes', '_Shoes', '_Jewelry']\n",
    "        # load data\n",
    "        self.train_data, self.train_data_aux, self.validate_data, self.test_data = self.readdata(dataset_list[dataset])\n",
    "        # load data for tensor factorization\n",
    "        self.train_record_aux, self.train_time_aux = self.readdata_time(dataset_list[dataset])\n",
    "        # load features\n",
    "        self.F = self.get_feature(dataset_list[dataset], Q) # CNN_AES Features\n",
    "        print(\"Populated the feature data...\")\n",
    "\n",
    "    def readdata(self, dataset):\n",
    "        #file paths\n",
    "        path_train = self.interactions_Jewelry_train\n",
    "        path_train_aux = self.interactions_Jewelry_train_aux\n",
    "        path_validate = self.interactions_Jewelry_validate\n",
    "        path_test = self.interactions_Jewelry_test\n",
    "        # read files\n",
    "        with open(path_train) as f:\n",
    "            line = f.readline()\n",
    "            train_data = json.loads(line)\n",
    "        f.close()\n",
    "        self.P = 0\n",
    "        self.Q = 0\n",
    "        for [u, i, r] in train_data:\n",
    "            if u > self.P:\n",
    "                self.P = u\n",
    "            if i > self.Q:\n",
    "                self.Q = i\n",
    "        with open(path_train_aux) as f:\n",
    "            line = f.readline()\n",
    "            train_data_aux = json.loads(line)\n",
    "        f.close()\n",
    "        with open(path_validate) as f:\n",
    "            line = f.readline()\n",
    "            validate_data = json.loads(line)\n",
    "        f.close()\n",
    "        with open(path_test) as f:\n",
    "            line = f.readline()\n",
    "            test_data = json.loads(line)\n",
    "        f.close()\n",
    "        return train_data, train_data_aux, validate_data, test_data # P: last user_id, last item_id\n",
    "\n",
    "    def readdata_time(self, dataset):\n",
    "        #file paths\n",
    "        path_train_record_aux = self.interactions_Jewelry_train_record\n",
    "        path_train_time_aux = self.interactions_Jewelry_train_time\n",
    "        # read files\n",
    "        with open(path_train_record_aux) as f:\n",
    "            line = f.readline()\n",
    "            train_record_aux = json.loads(line)\n",
    "        f.close()\n",
    "        with open(path_train_time_aux) as f:\n",
    "            line = f.readline()\n",
    "            train_time_aux = json.loads(line)\n",
    "        f.close()\n",
    "        self.Q = len(train_time_aux)\n",
    "        return train_record_aux, train_time_aux\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Generate the model\n",
    "        \"\"\"\n",
    "        self.model = DCFA(P=self.P, Q=self.Q, R=self.R, I=200, J=200, F=self.F, reg=1.5, mom=0.1)\n",
    "\n",
    "    #def create_negative(self):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start getting feature data...\n",
      "Start reading the interaction data dataset/interactions_Jewelry_train.json...\n",
      "Start reading the interaction data dataset/interactions_Jewelry_test.json...\n",
      "Debug:  [2, 1]\n",
      "Debug:  [8196, 3]\n",
      "Debug:  [6, 1]\n",
      "Debug:  [8206, 2]\n",
      "Debug:  [19, 1]\n",
      "Debug:  [22, 2]\n",
      "Debug:  [30, 2]\n",
      "Debug:  [8226, 5]\n",
      "Debug:  [8227, 1]\n",
      "Debug:  [36, 3]\n",
      "Debug:  [41, 1]\n",
      "Debug:  [2935, 1]\n",
      "Debug:  [15026, 1]\n",
      "Debug:  [5469, 1]\n",
      "Debug:  [8241, 2]\n",
      "Debug:  [8244, 1]\n",
      "Debug:  [8248, 1]\n",
      "Debug:  [57, 2]\n",
      "Debug:  [8251, 2]\n",
      "Debug:  [2741, 1]\n",
      "Debug:  [66, 1]\n",
      "Debug:  [12299, 1]\n",
      "Debug:  [8261, 2]\n",
      "Debug:  [8262, 1]\n",
      "Debug:  [1377, 2]\n",
      "Debug:  [12300, 1]\n",
      "Debug:  [8267, 2]\n",
      "Debug:  [80, 1]\n",
      "Debug:  [8280, 2]\n",
      "Debug:  [8281, 1]\n",
      "Debug:  [90, 3]\n",
      "Debug:  [8286, 1]\n",
      "Debug:  [96, 2]\n",
      "Debug:  [8208, 1]\n",
      "Debug:  [1382, 3]\n",
      "Debug:  [8296, 3]\n",
      "Debug:  [6844, 1]\n",
      "Debug:  [8300, 1]\n",
      "Debug:  [8301, 2]\n",
      "Debug:  [120, 3]\n",
      "Debug:  [123, 3]\n",
      "Debug:  [819, 1]\n",
      "Debug:  [127, 2]\n",
      "Debug:  [128, 3]\n",
      "Debug:  [8321, 1]\n",
      "Debug:  [130, 2]\n",
      "Debug:  [12310, 1]\n",
      "Debug:  [137, 1]\n",
      "Debug:  [141, 1]\n",
      "Debug:  [143, 3]\n",
      "Dimension of U:  (15923, 200)\n",
      "Dimension of V:  (238, 200)\n",
      "Dimension of W:  (238, 200)\n",
      "Dimension of T:  (0,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_3882/1675467469.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0munlimited_power\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mengine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_3882/3318836060.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, learning_rate, batch_size, k, feature)\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_interaction_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minteractions_Jewelry_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_interaction_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minteractions_Jewelry_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtrain_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_3882/3318836060.py\u001B[0m in \u001B[0;36mcreate_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    215\u001B[0m         \u001B[0mGenerate\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m         \"\"\"\n\u001B[0;32m--> 217\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDCFA\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mP\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mP\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mQ\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mQ\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mR\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mJ\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreg\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmom\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    218\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[0;31m#def create_negative(self):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_3882/3679933074.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, P, Q, R, I, J, F, reg, mom)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Dimension of T: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0;31m# Extract CNN-AES Features\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mK\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Dimension of F: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Size of F: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mK\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "unlimited_power = engine()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Shape:  (1, 3607)\n",
      "C Shape:  (1, 3607)\n",
      "A Shape:  (3607,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(), dtype=float64, numpy=0.06838241247835937>,\n <tf.Tensor: shape=(), dtype=float64, numpy=0.25331095412157273>,\n <tf.Tensor: shape=(), dtype=float64, numpy=0.2699544230745753>)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlimited_power.model.score(12308, 7, 182)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Shape:  (1, 3607)\n",
      "C Shape:  (1, 3607)\n",
      "A Shape:  (3607,)\n",
      "B Shape:  (1, 3607)\n",
      "C Shape:  (1, 3607)\n",
      "A Shape:  (3607,)\n",
      "B_ij:  tf.Tensor(0.041065853788441437, shape=(), dtype=float64)\n",
      "C_ij:  tf.Tensor(0.040162517684509236, shape=(), dtype=float64)\n",
      "A loss:  -0.6823751915501121\n",
      "B loss:  -0.6728250393985218\n",
      "C loss:  -0.6732675366461449\n"
     ]
    },
    {
     "data": {
      "text/plain": "-0.8169844491545787"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlimited_power.model.call(12308, 0, negative_item, 182)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}